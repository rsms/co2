<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>rt-go-sched-parking</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="/style.css?0">
</head>
<body>
<h1><a id="go-scheduler-worker-thread-parking-unparking" class="anchor" aria-hidden="true" href="#go-scheduler-worker-thread-parking-unparking"></a>Go scheduler Worker thread parking/unparking</h1>
<p>We need to balance between keeping enough running worker threads to utilize
available hardware parallelism and parking excessive running worker threads
to conserve CPU resources and power. This is not simple for two reasons:
(1) scheduler state is intentionally distributed (in particular, per-P work
queues), so it is not possible to compute global predicates on fast paths;
(2) for optimal thread management we would need to know the future (don't park
a worker thread when a new goroutine will be readied in near future).</p>
<p>Three rejected approaches that would work badly:</p>
<ol>
<li><p>Centralize all scheduler state (would inhibit scalability).</p>
</li>
<li><p>Direct goroutine handoff. That is, when we ready a new goroutine and there
is a spare P, unpark a thread and handoff it the thread and the goroutine.
This would lead to thread state thrashing, as the thread that readied the
goroutine can be out of work the very next moment, we will need to park it.
Also, it would destroy locality of computation as we want to preserve
dependent goroutines on the same thread; and introduce additional latency.</p>
</li>
<li><p>Unpark an additional thread whenever we ready a goroutine and there is an
idle P, but don't do handoff. This would lead to excessive thread parking/
unparking as the additional threads will instantly park without discovering
any work to do.</p>
</li>
</ol>
<p>The current approach:</p>
<p>We unpark an additional thread when we ready a goroutine if (1) there is an
idle P and there are no &quot;spinning&quot; worker threads. A worker thread is considered
spinning if it is out of local work and did not find work in global run queue/
netpoller; the spinning state is denoted in m.spinning and in sched.nmspinning.
Threads unparked this way are also considered spinning; we don't do goroutine
handoff so such threads are out of work initially. Spinning threads do some
spinning looking for work in per-P run queues before parking. If a spinning
thread finds work it takes itself out of the spinning state and proceeds to
execution. If it does not find work it takes itself out of the spinning state
and then parks.</p>
<p>If there is at least one spinning thread (sched.nmspinning&gt;1), we don't unpark
new threads when readying goroutines. To compensate for that, if the last spinning
thread finds work and stops spinning, it must unpark a new spinning thread.
This approach smooths out unjustified spikes of thread unparking,
but at the same time guarantees eventual maximal CPU parallelism utilization.</p>
<p>The main implementation complication is that we need to be very careful during
spinning→non-spinning thread transition. This transition can race with submission
of a new goroutine, and either one part or another needs to unpark another worker
thread. If they both fail to do that, we can end up with semi-persistent CPU
underutilization. The general pattern for goroutine readying is: submit a goroutine
to local work queue, StoreLoad-style memory barrier, check sched.nmspinning.</p>
<p>The general pattern for spinning→non-spinning transition is:</p>
<ul>
<li>decrement nmspinning,</li>
<li>StoreLoad-style memory barrier,</li>
<li>check all per-P work queues for new work.</li>
</ul>
<p>Note that all this complexity does not apply to global run queue as we are not
sloppy about thread unparking when submitting to global queue. Also see comments
for nmspinning manipulation.</p>

</body>
</html>
